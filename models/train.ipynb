{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee8d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b3b25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = 10\n",
    "in_sample = \"data/test.pkl\"\n",
    "out_file = \"results/test.torch\"\n",
    "out_path = \"plots/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d22601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data into memory...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Data into memory...\")\n",
    "data = pickle.load( open( in_sample , \"rb\" ) )\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8edb1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.pre_norm_Q = nn.LayerNorm(embed_dim)\n",
    "        self.pre_norm_K = nn.LayerNorm(embed_dim)\n",
    "        self.pre_norm_V = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim,num_heads=num_heads,batch_first=True, dropout=0.25)\n",
    "        self.post_norm = nn.LayerNorm(embed_dim)\n",
    "        self.out = nn.Linear(embed_dim,embed_dim)\n",
    "    def forward(self, Query, Key, Value):\n",
    "        Query = self.pre_norm_Q(Query)\n",
    "        Key = self.pre_norm_K(Key)\n",
    "        Value = self.pre_norm_V(Value)\n",
    "        context, weights = self.attention(Query, Key, Value)\n",
    "        context = self.post_norm(context)\n",
    "        latent = Query + context\n",
    "        tmp = F.gelu(self.out(latent))\n",
    "        latent = latent + tmp\n",
    "        return latent, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dbb413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.embed_dim = 256\n",
    "        self.num_heads = 8\n",
    "        self.num_jet_feats = 4\n",
    "        self.num_trk_feats = 6\n",
    "\n",
    "        self.jet_initializer = nn.Linear(self.num_jet_feats, self.embed_dim)\n",
    "        self.jet_trk_initializer = nn.Linear(self.num_trk_feats, self.embed_dim)\n",
    "        self.trk_initializer = nn.Linear(self.num_trk_feats, self.embed_dim)\n",
    "\n",
    "        # Track Encoder Stack\n",
    "        self.trk_encoder1 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.trk_encoder2 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.trk_encoder3 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.jet_postprocess = nn.Linear(self.embed_dim*2, self.embed_dim)\n",
    "\n",
    "        # All Track Encoder Stack\n",
    "        self.all_trk_encoder1 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.all_trk_encoder2 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.all_trk_encoder3 = Encoder(self.embed_dim, self.num_heads)\n",
    "\n",
    "        # Cross Encoder Stack\n",
    "        self.cross_encoder1 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.cross_encoder2 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.cross_encoder3 = Encoder(self.embed_dim, self.num_heads)\n",
    "\n",
    "        # Jet Encoder Stack\n",
    "        self.jet_encoder1 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.jet_encoder2 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.jet_encoder3 = Encoder(self.embed_dim, self.num_heads)\n",
    "\n",
    "        # Regression Task\n",
    "        self.regression = nn.Linear(self.embed_dim, 2)\n",
    "        \n",
    "        # BCE Task\n",
    "        self.classification = nn.Linear(self.embed_dim, 1)\n",
    "\n",
    "    def forward(self, jets, jet_trks, trks):\n",
    "        # Feature preprocessing layers\n",
    "        jet_init = F.relu(self.jet_initializer(jets))\n",
    "        jet_trk_init = F.relu(self.jet_trk_initializer(jet_trks))\n",
    "        trk_init = F.relu(self.trk_initializer(trks))\n",
    "\n",
    "        # Calculate aggregated tracks using attention\n",
    "        jet_trk_embedding, trk_weights = self.trk_encoder1(jet_trk_init, jet_trk_init, jet_trk_init)\n",
    "        jet_trk_embedding, trk_weights = self.trk_encoder2(jet_trk_embedding, jet_trk_embedding, jet_trk_embedding)\n",
    "        jet_trk_embedding, trk_weights = self.trk_encoder3(jet_trk_embedding, jet_trk_embedding, jet_trk_embedding)\n",
    "\n",
    "        # Generate meaningful jet_embedding using info from trk_aggregated\n",
    "        jet_trk_aggregated = jet_trk_embedding.sum(dim=1)\n",
    "        jet_embedding = torch.cat((jet_init, jet_trk_aggregated),1)\n",
    "        jet_embedding = F.relu(self.jet_postprocess(jet_embedding))\n",
    "\n",
    "        # All Track Attention\n",
    "        all_trk_embedding, all_trk_weights = self.all_trk_encoder1(trk_init, trk_init, trk_init)\n",
    "        all_trk_embedding, all_trk_weights = self.all_trk_encoder2(all_trk_embedding, all_trk_embedding, all_trk_embedding)\n",
    "        all_trk_embedding, all_trk_weights = self.all_trk_encoder3(all_trk_embedding, all_trk_embedding, all_trk_embedding)\n",
    "\n",
    "        # Cross Attention\n",
    "        jet_embedding, cross_weights = self.cross_encoder1(jet_embedding, all_trk_embedding, all_trk_embedding)\n",
    "        jet_embedding, cross_weights = self.cross_encoder2(jet_embedding, all_trk_embedding, all_trk_embedding)\n",
    "        jet_embedding, cross_weights = self.cross_encoder3(jet_embedding, all_trk_embedding, all_trk_embedding)\n",
    "\n",
    "        # Update embeddings of jets in the contex of entire event\n",
    "        jet_embedding, jet_weights = self.jet_encoder1(jet_embedding, jet_embedding, jet_embedding)\n",
    "        jet_embedding, jet_weights = self.jet_encoder2(jet_embedding, jet_embedding, jet_embedding)\n",
    "        jet_embedding, jet_weights = self.jet_encoder3(jet_embedding, jet_embedding, jet_embedding)\n",
    "\n",
    "        # Get Jet output\n",
    "        jet_output = F.sigmoid(self.regression(jet_embedding))\n",
    "        \n",
    "        # Get Track output\n",
    "        trk_output = F.sigmoid(self.classification(all_trk_embedding))\n",
    "\n",
    "        return jet_output, trk_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5447b082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (jet_initializer): Linear(in_features=4, out_features=256, bias=True)\n",
      "  (jet_trk_initializer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (trk_initializer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (trk_encoder1): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (trk_encoder2): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (trk_encoder3): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (jet_postprocess): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (all_trk_encoder1): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (all_trk_encoder2): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (all_trk_encoder3): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (cross_encoder1): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (cross_encoder2): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (cross_encoder3): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (jet_encoder1): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (jet_encoder2): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (jet_encoder3): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (regression): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (classification): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "torch.Size([8, 2])\n",
      "torch.Size([114, 1])\n",
      "torch.Size([8, 2])\n",
      "torch.Size([114, 1])\n",
      "tensor(0.1713, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6433, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get Instance of the model\n",
    "model = Model()\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# Pass some data to the model and print outputs\n",
    "Event_no = 0\n",
    "Jets    = 0\n",
    "Trk_Jet = 1\n",
    "Trks     = 2\n",
    "\n",
    "jet_pred, trk_pred = model(X_train[Event_no][Jets],X_train[Event_no][Trk_Jet],X_train[Event_no][Trks])\n",
    "\n",
    "print(jet_pred.shape)\n",
    "print(trk_pred.shape)\n",
    "\n",
    "# Evaluate the loss\n",
    "jet_loss_fn = nn.MSELoss()\n",
    "trk_loss_fn = nn.BCELoss()\n",
    "\n",
    "print(y_train[Event_no][0].shape)\n",
    "print(y_train[Event_no][1].shape)\n",
    "\n",
    "\n",
    "print(jet_loss_fn(jet_pred,y_train[Event_no][0]))\n",
    "\n",
    "print(trk_loss_fn(trk_pred,y_train[Event_no][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8f8340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Training Loop\n",
    "def train(model, optimizer, X_train, y_train, X_val, y_val, epochs=40):\n",
    "\n",
    "    combined_history = []\n",
    "\n",
    "    num_train = len(X_train)\n",
    "    num_val = len(X_val)\n",
    "\n",
    "    step_size=15\n",
    "    gamma=0.1\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        cumulative_loss_train = 0\n",
    "\n",
    "        for i in range(num_train):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            jet_pred, trk_pred = model(X_train[i][0].to(device), X_train[i][1].to(device), X_train[i][2].to(device))\n",
    "\n",
    "            jet_loss=jet_loss_fn(jet_pred, y_train[i][0].to(device))\n",
    "            jet_loss=jet_loss_fn(jet_pred, y_train[i][0].to(device))\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cumulative_loss_train+=loss.detach().cpu().numpy().mean()\n",
    "\n",
    "        cumulative_loss_train = cumulative_loss_train / num_train\n",
    "\n",
    "        model.eval()\n",
    "        cumulative_loss_val = 0\n",
    "        for i in range(num_val):\n",
    "            jet_pred, trk_pred = model(X_val[i][0].to(device), X_val[i][1].to(device), X_val[i][2].to(device))\n",
    "            loss=loss_fn(output, y_val[i].to(device))\n",
    "\n",
    "            cumulative_loss_val+=loss.detach().cpu().numpy().mean()\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        cumulative_loss_val = cumulative_loss_val / num_val\n",
    "        combined_history.append([cumulative_loss_train, cumulative_loss_val])\n",
    "\n",
    "        if e%1==0:\n",
    "            print('Epoch:',e,'\\tTrain Loss:',round(cumulative_loss_train,6),'\\tVal Loss:',round(cumulative_loss_val,6))\n",
    "\n",
    "        if (e+1)%step_size==0:\n",
    "            print(\"\\tReducing Step Size by \", gamma)\n",
    "\n",
    "    return np.array(combined_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be2be03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available: \", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "417f11ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[1;32m      8\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m---> 10\u001b[0m combined_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEpochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model,out_file)\n",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, X_train, y_train, X_val, y_val, epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     19\u001b[0m jet_pred, trk_pred \u001b[38;5;241m=\u001b[39m model(X_train[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), X_train[i][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), X_train[i][\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m---> 21\u001b[0m loss\u001b[38;5;241m=\u001b[39mloss_fn(\u001b[43moutput\u001b[49m, y_train[i]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "# ### Train Model\n",
    "\n",
    "model = Model()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "combined_history = train(model, optimizer, X_train, y_train, X_val, y_val,epochs=Epochs)\n",
    "\n",
    "torch.save(model,out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1774409",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(combined_history[:,0], label=\"Train\")\n",
    "plt.plot(combined_history[:,1], label=\"Val\")\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.savefig(out_path+\"/Loss_Curve.png\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d3e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Evaluate Model\n",
    "\n",
    "model.eval()\n",
    "cumulative_loss_test = 0\n",
    "cumulative_MSE_test = 0\n",
    "cumulative_BCE_test = 0\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "binary_pred = []\n",
    "binary_true = []\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_test = len(X_test)\n",
    "for i in range(num_test):\n",
    "    jet_pred, trk_pred = model(X_test[i][0].to(device), X_test[i][1].to(device), X_test[i][2].to(device))\n",
    "\n",
    "    loss=loss_fn(output, y_test[i].to(device))\n",
    "    cumulative_loss_test+=loss.detach().cpu().numpy().mean()\n",
    "\n",
    "    for j in range(output.shape[0]):\n",
    "        predicted_labels.append(float(output[j][0].detach().cpu().numpy()))\n",
    "        true_labels.append(float(y_test[i][j][0].detach().numpy()))\n",
    "\n",
    "cumulative_loss_test = cumulative_loss_test / num_test\n",
    "\n",
    "print(\"Train Loss:\\t\", combined_history[-1][0])\n",
    "print(\"Val Loss:\\t\", combined_history[-1][1])\n",
    "print(\"Test Loss:\\t\", cumulative_loss_test)\n",
    "print()\n",
    "print(\"Test MAE:\\t\", mean_absolute_error(true_labels, predicted_labels))\n",
    "print(\"Test RMSE:\\t\", root_mean_squared_error(true_labels, predicted_labels))\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(true_labels,histtype='step',color='r',label='True Distribution',bins=50,range=(0,1))\n",
    "plt.hist(predicted_labels,histtype='step',color='b',label='Predicted Distribution',bins=50,range=(0,1))\n",
    "plt.title(\"Predicted Ouput Distribution using Attention Model (\\u03BC=60)\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('PU Fraction',loc='right')\n",
    "plt.savefig(out_path+\"/pred_1d.png\")\n",
    "#plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Ouput Distribution using Attention Model (\\u03BC=60)\")\n",
    "plt.hist2d(predicted_labels,true_labels, bins=100,norm=mcolors.PowerNorm(0.2))\n",
    "plt.xlabel('Predicted PU Fraction',loc='right')\n",
    "plt.ylabel('True PU Fraction',loc='top')\n",
    "plt.savefig(out_path+\"/pred_2d.png\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a8f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ATLAS_roc(y_true, y_pred):\n",
    "    sig = (y_true==1)\n",
    "    bkg = ~sig\n",
    "\n",
    "    sig_eff = []\n",
    "    bkg_eff = []\n",
    "\n",
    "    thresholds = np.linspace(0,0.999,100)\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        sig_eff.append(((y_pred[sig] > threshold).sum() / y_true[sig].shape[0]))\n",
    "        bkg_eff.append(((y_pred[bkg] > threshold).sum()  / y_true[bkg].shape[0]))\n",
    "\n",
    "    bkg_rej = [1/x for x in bkg_eff]\n",
    "    return np.array(sig_eff), np.array(bkg_rej), thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68494175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc(y_true, y_pred):\n",
    "    sig = (y_true==1)\n",
    "    bkg = ~sig\n",
    "\n",
    "    sig_eff = []\n",
    "    fake_rate = []\n",
    "\n",
    "    thresholds = np.linspace(0,0.97,100)\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        sig_eff.append(((y_pred[sig] > threshold).sum() / y_true[sig].shape[0]))\n",
    "        fake_rate.append(((y_pred[bkg] > threshold).sum()  / y_true[bkg].shape[0]))\n",
    "\n",
    "    return np.array(sig_eff), np.array(fake_rate), thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6baa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred, threshold):\n",
    "    y_Pred = np.array(y_pred > threshold).astype(int)\n",
    "    y_True = np.array(y_true > threshold).astype(int)\n",
    "    x1,y1, thresholds1 = ATLAS_roc(y_True, y_pred)\n",
    "    x2,y2, thresholds2 = roc(y_True, y_pred)\n",
    "    AUC = roc_auc_score(y_True, y_Pred)\n",
    "    BA = accuracy_score(y_True, y_Pred)\n",
    "    f1 = f1_score(y_True, y_Pred)\n",
    "    return x1,y1,x2,y2,thresholds1,thresholds2,AUC,BA,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49263cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2,1,figsize=(16,9), gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "x1,y1,x1_v2,y1_v2,th1,th1_v2,AUC1,BA1,f11 = get_metrics(np.array(true_labels), np.array(predicted_labels), 0.7)\n",
    "\n",
    "ax1.set_title(\"ATLAS ROC Curve\")\n",
    "ax1.set_xlabel(\"sig eff\",loc='right')\n",
    "ax1.set_ylabel(\"bkg rej\")\n",
    "\n",
    "ax1.plot(x1,y1, label=\"Attention\",color='m')\n",
    "AUC1 = \"Attention Model AUC: \" + str(round(AUC1,4))\n",
    "ax1.text(0.41,8,AUC1)\n",
    "\n",
    "x = 1-np.flip(th1)\n",
    "ratio1 = np.interp(x,np.flip(x1),np.flip(y1))/np.interp(x,np.flip(x1),np.flip(y1))\n",
    "ax2.plot(x,ratio1,linestyle='--',color='m')\n",
    "\n",
    "# General Plot Settings\n",
    "ax1.legend()\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(which='both')\n",
    "ax2.grid(which='both')\n",
    "ax1.set_xlim(0.3,1)\n",
    "ax2.set_xlim(0.3,1)\n",
    "plt.savefig(out_path+\"/ATLAS_ROC.png\")\n",
    "#plt.show()\n",
    "\n",
    "print(\"\\tBinary Accuracy: \", BA1, \"\\tF1 Score: \", f11)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7cbdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_names = [\"results/MfracNN_Results.np\"]\n",
    "#results1 = np.concatenate((x1_v2[np.newaxis],y1_v2[np.newaxis],th1_v2[np.newaxis]),axis=0)\n",
    "#results = [results1]\n",
    "#for i, file in enumerate(file_names):\n",
    "#    with open(file, 'wb') as f:\n",
    "#        np.save(f, results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a39976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)\n",
    "print(\"Trainable Parameters :\", np.sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "#model = torch.load(\"PUFNN.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1121c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ### Visualize Attention Scores\n",
    "\n",
    "plot_events=5\n",
    "for i in range(plot_events):\n",
    "    # Get Model Predictions\n",
    "    model.eval()\n",
    "    output, jet_weights, trk_weights, cross_weights = model(X_test[i][0].to(device), X_test[i][1].to(device), X_test[i][2].to(device))\n",
    "\n",
    "    num_jets = len(output)\n",
    "\n",
    "    ticks = [x for x in range(num_jets)]\n",
    "    jet_labels= [round(float(label.detach().cpu().numpy()[0]),2) for label in y_test[i]]\n",
    "\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(15,15))\n",
    "    im0 = ax1.imshow(jet_weights.detach().cpu().numpy())\n",
    "    ax1.set_xticks(ticks, jet_labels)\n",
    "    ax1.set_yticks(ticks, jet_labels)\n",
    "    ax1.set_title(\"Attention Scores Between Jets and tracks\")\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im0, cax=cax, orientation='vertical')\n",
    "\"\"\"\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
