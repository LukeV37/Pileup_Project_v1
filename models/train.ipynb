{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce03f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19af1953",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = 10\n",
    "in_sample = \"data/test.pkl\"\n",
    "out_file = \"results/test.torch\"\n",
    "out_path = \"plots/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75992afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data into memory...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Data into memory...\")\n",
    "data = pickle.load( open( in_sample , \"rb\" ) )\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de5e1382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.pre_norm_Q = nn.LayerNorm(embed_dim)\n",
    "        self.pre_norm_K = nn.LayerNorm(embed_dim)\n",
    "        self.pre_norm_V = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim,num_heads=num_heads,batch_first=True, dropout=0.25)\n",
    "        self.post_norm = nn.LayerNorm(embed_dim)\n",
    "        self.out = nn.Linear(embed_dim,embed_dim)\n",
    "    def forward(self, Query, Key, Value):\n",
    "        Query = self.pre_norm_Q(Query)\n",
    "        Key = self.pre_norm_K(Key)\n",
    "        Value = self.pre_norm_V(Value)\n",
    "        context, weights = self.attention(Query, Key, Value)\n",
    "        context = self.post_norm(context)\n",
    "        latent = Query + context\n",
    "        tmp = F.gelu(self.out(latent))\n",
    "        latent = latent + tmp\n",
    "        return latent, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa02e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.embed_dim = 256\n",
    "        self.num_heads = 8\n",
    "        self.num_jet_feats = 4\n",
    "        self.num_trk_feats = 6\n",
    "\n",
    "        self.jet_initializer = nn.Linear(self.num_jet_feats, self.embed_dim)\n",
    "        self.jet_trk_initializer = nn.Linear(self.num_trk_feats, self.embed_dim)\n",
    "        self.trk_initializer = nn.Linear(self.num_trk_feats, self.embed_dim)\n",
    "\n",
    "        # Track Encoder Stack\n",
    "        self.trk_encoder1 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.trk_encoder2 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.trk_encoder3 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.jet_postprocess = nn.Linear(self.embed_dim*2, self.embed_dim)\n",
    "\n",
    "        # All Track Encoder Stack\n",
    "        self.all_trk_encoder1 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.all_trk_encoder2 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.all_trk_encoder3 = Encoder(self.embed_dim, self.num_heads)\n",
    "\n",
    "        # Cross Encoder Stack\n",
    "        self.cross_encoder1 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.cross_encoder2 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.cross_encoder3 = Encoder(self.embed_dim, self.num_heads)\n",
    "\n",
    "        # Jet Encoder Stack\n",
    "        self.jet_encoder1 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.jet_encoder2 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.jet_encoder3 = Encoder(self.embed_dim, self.num_heads)\n",
    "\n",
    "        # Regression Task\n",
    "        self.regression = nn.Linear(self.embed_dim, 2)\n",
    "        \n",
    "        # BCE Task\n",
    "        self.classification = nn.Linear(self.embed_dim, 1)\n",
    "\n",
    "    def forward(self, jets, jet_trks, trks):\n",
    "        # Feature preprocessing layers\n",
    "        jet_init = F.relu(self.jet_initializer(jets))\n",
    "        jet_trk_init = F.relu(self.jet_trk_initializer(jet_trks))\n",
    "        trk_init = F.relu(self.trk_initializer(trks))\n",
    "\n",
    "        # Calculate aggregated tracks using attention\n",
    "        jet_trk_embedding, trk_weights = self.trk_encoder1(jet_trk_init, jet_trk_init, jet_trk_init)\n",
    "        jet_trk_embedding, trk_weights = self.trk_encoder2(jet_trk_embedding, jet_trk_embedding, jet_trk_embedding)\n",
    "        jet_trk_embedding, trk_weights = self.trk_encoder3(jet_trk_embedding, jet_trk_embedding, jet_trk_embedding)\n",
    "\n",
    "        # Generate meaningful jet_embedding using info from trk_aggregated\n",
    "        jet_trk_aggregated = jet_trk_embedding.sum(dim=1)\n",
    "        jet_embedding = torch.cat((jet_init, jet_trk_aggregated),1)\n",
    "        jet_embedding = F.relu(self.jet_postprocess(jet_embedding))\n",
    "\n",
    "        # All Track Attention\n",
    "        all_trk_embedding, all_trk_weights = self.all_trk_encoder1(trk_init, trk_init, trk_init)\n",
    "        all_trk_embedding, all_trk_weights = self.all_trk_encoder2(all_trk_embedding, all_trk_embedding, all_trk_embedding)\n",
    "        all_trk_embedding, all_trk_weights = self.all_trk_encoder3(all_trk_embedding, all_trk_embedding, all_trk_embedding)\n",
    "\n",
    "        # Cross Attention\n",
    "        jet_embedding, cross_weights = self.cross_encoder1(jet_embedding, all_trk_embedding, all_trk_embedding)\n",
    "        jet_embedding, cross_weights = self.cross_encoder2(jet_embedding, all_trk_embedding, all_trk_embedding)\n",
    "        jet_embedding, cross_weights = self.cross_encoder3(jet_embedding, all_trk_embedding, all_trk_embedding)\n",
    "\n",
    "        # Update embeddings of jets in the contex of entire event\n",
    "        jet_embedding, jet_weights = self.jet_encoder1(jet_embedding, jet_embedding, jet_embedding)\n",
    "        jet_embedding, jet_weights = self.jet_encoder2(jet_embedding, jet_embedding, jet_embedding)\n",
    "        jet_embedding, jet_weights = self.jet_encoder3(jet_embedding, jet_embedding, jet_embedding)\n",
    "\n",
    "        # Get Jet output\n",
    "        jet_output = F.sigmoid(self.regression(jet_embedding))\n",
    "        \n",
    "        # Get Track output\n",
    "        trk_output = F.sigmoid(self.classification(all_trk_embedding))\n",
    "\n",
    "        return jet_output, trk_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5e051a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (jet_initializer): Linear(in_features=4, out_features=256, bias=True)\n",
      "  (jet_trk_initializer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (trk_initializer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (trk_encoder1): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (trk_encoder2): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (trk_encoder3): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (jet_postprocess): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (all_trk_encoder1): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (all_trk_encoder2): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (all_trk_encoder3): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (cross_encoder1): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (cross_encoder2): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (cross_encoder3): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (jet_encoder1): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (jet_encoder2): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (jet_encoder3): Encoder(\n",
      "    (pre_norm_Q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_K): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_norm_V): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (regression): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (classification): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "torch.Size([8, 2])\n",
      "torch.Size([114, 1])\n",
      "torch.Size([8, 2])\n",
      "torch.Size([114, 1])\n",
      "tensor(0.1713, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6433, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get Instance of the model\n",
    "model = Model()\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# Pass some data to the model and print outputs\n",
    "Event_no = 0\n",
    "Jets    = 0\n",
    "Trk_Jet = 1\n",
    "Trks     = 2\n",
    "\n",
    "jet_pred, trk_pred = model(X_train[Event_no][Jets],X_train[Event_no][Trk_Jet],X_train[Event_no][Trks])\n",
    "\n",
    "print(jet_pred.shape)\n",
    "print(trk_pred.shape)\n",
    "\n",
    "# Evaluate the loss\n",
    "jet_loss_fn = nn.MSELoss()\n",
    "trk_loss_fn = nn.BCELoss()\n",
    "\n",
    "print(y_train[Event_no][0].shape)\n",
    "print(y_train[Event_no][1].shape)\n",
    "\n",
    "\n",
    "print(jet_loss_fn(jet_pred,y_train[Event_no][0]))\n",
    "\n",
    "print(trk_loss_fn(trk_pred,y_train[Event_no][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d37ed976",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Training Loop\n",
    "def train(model, optimizer, X_train, y_train, X_val, y_val, epochs=40):\n",
    "\n",
    "    combined_history = []\n",
    "\n",
    "    num_train = len(X_train)\n",
    "    num_val = len(X_val)\n",
    "\n",
    "    step_size=15\n",
    "    gamma=0.1\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        cumulative_loss_train = 0\n",
    "\n",
    "        for i in range(num_train):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            jet_pred, trk_pred = model(X_train[i][0].to(device), X_train[i][1].to(device), X_train[i][2].to(device))\n",
    "\n",
    "            jet_loss=jet_loss_fn(jet_pred, y_train[i][0].to(device))\n",
    "            trk_loss=trk_loss_fn(trk_pred, y_train[i][1].to(device))\n",
    "\n",
    "            loss = jet_loss+trk_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cumulative_loss_train+=loss.detach().cpu().numpy().mean()\n",
    "\n",
    "        cumulative_loss_train = cumulative_loss_train / num_train\n",
    "\n",
    "        model.eval()\n",
    "        cumulative_loss_val = 0\n",
    "        for i in range(num_val):\n",
    "            jet_pred, trk_pred = model(X_val[i][0].to(device), X_val[i][1].to(device), X_val[i][2].to(device))\n",
    "            \n",
    "            jet_loss=jet_loss_fn(jet_pred, y_val[i][0].to(device))\n",
    "            trk_loss=trk_loss_fn(trk_pred, y_val[i][1].to(device))\n",
    "\n",
    "            loss = jet_loss+trk_loss\n",
    "\n",
    "            cumulative_loss_val+=loss.detach().cpu().numpy().mean()\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        cumulative_loss_val = cumulative_loss_val / num_val\n",
    "        combined_history.append([cumulative_loss_train, cumulative_loss_val])\n",
    "\n",
    "        if e%1==0:\n",
    "            print('Epoch:',e,'\\tTrain Loss:',round(cumulative_loss_train,6),'\\tVal Loss:',round(cumulative_loss_val,6))\n",
    "\n",
    "        if (e+1)%step_size==0:\n",
    "            print(\"\\tReducing Step Size by \", gamma)\n",
    "\n",
    "    return np.array(combined_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "424441f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available: \", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac711bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTrain Loss: 0.507708 \tVal Loss: 0.377057\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[1;32m      8\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m---> 10\u001b[0m combined_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEpochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model,out_file)\n",
      "Cell \u001b[0;32mIn[14], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, X_train, y_train, X_val, y_val, epochs)\u001b[0m\n\u001b[1;32m     24\u001b[0m     loss \u001b[38;5;241m=\u001b[39m jet_loss\u001b[38;5;241m+\u001b[39mtrk_loss\n\u001b[1;32m     26\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 27\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     cumulative_loss_train\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     31\u001b[0m cumulative_loss_train \u001b[38;5;241m=\u001b[39m cumulative_loss_train \u001b[38;5;241m/\u001b[39m num_train\n",
      "File \u001b[0;32m~/VirtualEnvs/torch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:140\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    139\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VirtualEnvs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/VirtualEnvs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/VirtualEnvs/torch/lib/python3.10/site-packages/torch/optim/adamw.py:243\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    230\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    232\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    233\u001b[0m         group,\n\u001b[1;32m    234\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m         state_steps,\n\u001b[1;32m    241\u001b[0m     )\n\u001b[0;32m--> 243\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/VirtualEnvs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VirtualEnvs/torch/lib/python3.10/site-packages/torch/optim/adamw.py:875\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 875\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VirtualEnvs/torch/lib/python3.10/site-packages/torch/optim/adamw.py:405\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    402\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[0;32m--> 405\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m device \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    409\u001b[0m device \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ### Train Model\n",
    "\n",
    "model = Model()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "combined_history = train(model, optimizer, X_train, y_train, X_val, y_val,epochs=Epochs)\n",
    "\n",
    "torch.save(model,out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b10d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(combined_history[:,0], label=\"Train\")\n",
    "plt.plot(combined_history[:,1], label=\"Val\")\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.savefig(out_path+\"/Loss_Curve.png\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Evaluate Model\n",
    "\n",
    "model.eval()\n",
    "cumulative_loss_test = 0\n",
    "cumulative_MSE_test = 0\n",
    "cumulative_BCE_test = 0\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "binary_pred = []\n",
    "binary_true = []\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_test = len(X_test)\n",
    "for i in range(num_test):\n",
    "    jet_pred, trk_pred = model(X_test[i][0].to(device), X_test[i][1].to(device), X_test[i][2].to(device))\n",
    "\n",
    "    loss=loss_fn(output, y_test[i].to(device))\n",
    "    cumulative_loss_test+=loss.detach().cpu().numpy().mean()\n",
    "\n",
    "    for j in range(output.shape[0]):\n",
    "        predicted_labels.append(float(output[j][0].detach().cpu().numpy()))\n",
    "        true_labels.append(float(y_test[i][j][0].detach().numpy()))\n",
    "\n",
    "cumulative_loss_test = cumulative_loss_test / num_test\n",
    "\n",
    "print(\"Train Loss:\\t\", combined_history[-1][0])\n",
    "print(\"Val Loss:\\t\", combined_history[-1][1])\n",
    "print(\"Test Loss:\\t\", cumulative_loss_test)\n",
    "print()\n",
    "print(\"Test MAE:\\t\", mean_absolute_error(true_labels, predicted_labels))\n",
    "print(\"Test RMSE:\\t\", root_mean_squared_error(true_labels, predicted_labels))\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(true_labels,histtype='step',color='r',label='True Distribution',bins=50,range=(0,1))\n",
    "plt.hist(predicted_labels,histtype='step',color='b',label='Predicted Distribution',bins=50,range=(0,1))\n",
    "plt.title(\"Predicted Ouput Distribution using Attention Model (\\u03BC=60)\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('PU Fraction',loc='right')\n",
    "plt.savefig(out_path+\"/pred_1d.png\")\n",
    "#plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Ouput Distribution using Attention Model (\\u03BC=60)\")\n",
    "plt.hist2d(predicted_labels,true_labels, bins=100,norm=mcolors.PowerNorm(0.2))\n",
    "plt.xlabel('Predicted PU Fraction',loc='right')\n",
    "plt.ylabel('True PU Fraction',loc='top')\n",
    "plt.savefig(out_path+\"/pred_2d.png\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a379a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ATLAS_roc(y_true, y_pred):\n",
    "    sig = (y_true==1)\n",
    "    bkg = ~sig\n",
    "\n",
    "    sig_eff = []\n",
    "    bkg_eff = []\n",
    "\n",
    "    thresholds = np.linspace(0,0.999,100)\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        sig_eff.append(((y_pred[sig] > threshold).sum() / y_true[sig].shape[0]))\n",
    "        bkg_eff.append(((y_pred[bkg] > threshold).sum()  / y_true[bkg].shape[0]))\n",
    "\n",
    "    bkg_rej = [1/x for x in bkg_eff]\n",
    "    return np.array(sig_eff), np.array(bkg_rej), thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb44041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc(y_true, y_pred):\n",
    "    sig = (y_true==1)\n",
    "    bkg = ~sig\n",
    "\n",
    "    sig_eff = []\n",
    "    fake_rate = []\n",
    "\n",
    "    thresholds = np.linspace(0,0.97,100)\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        sig_eff.append(((y_pred[sig] > threshold).sum() / y_true[sig].shape[0]))\n",
    "        fake_rate.append(((y_pred[bkg] > threshold).sum()  / y_true[bkg].shape[0]))\n",
    "\n",
    "    return np.array(sig_eff), np.array(fake_rate), thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aedf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred, threshold):\n",
    "    y_Pred = np.array(y_pred > threshold).astype(int)\n",
    "    y_True = np.array(y_true > threshold).astype(int)\n",
    "    x1,y1, thresholds1 = ATLAS_roc(y_True, y_pred)\n",
    "    x2,y2, thresholds2 = roc(y_True, y_pred)\n",
    "    AUC = roc_auc_score(y_True, y_Pred)\n",
    "    BA = accuracy_score(y_True, y_Pred)\n",
    "    f1 = f1_score(y_True, y_Pred)\n",
    "    return x1,y1,x2,y2,thresholds1,thresholds2,AUC,BA,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661625d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2,1,figsize=(16,9), gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "x1,y1,x1_v2,y1_v2,th1,th1_v2,AUC1,BA1,f11 = get_metrics(np.array(true_labels), np.array(predicted_labels), 0.7)\n",
    "\n",
    "ax1.set_title(\"ATLAS ROC Curve\")\n",
    "ax1.set_xlabel(\"sig eff\",loc='right')\n",
    "ax1.set_ylabel(\"bkg rej\")\n",
    "\n",
    "ax1.plot(x1,y1, label=\"Attention\",color='m')\n",
    "AUC1 = \"Attention Model AUC: \" + str(round(AUC1,4))\n",
    "ax1.text(0.41,8,AUC1)\n",
    "\n",
    "x = 1-np.flip(th1)\n",
    "ratio1 = np.interp(x,np.flip(x1),np.flip(y1))/np.interp(x,np.flip(x1),np.flip(y1))\n",
    "ax2.plot(x,ratio1,linestyle='--',color='m')\n",
    "\n",
    "# General Plot Settings\n",
    "ax1.legend()\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(which='both')\n",
    "ax2.grid(which='both')\n",
    "ax1.set_xlim(0.3,1)\n",
    "ax2.set_xlim(0.3,1)\n",
    "plt.savefig(out_path+\"/ATLAS_ROC.png\")\n",
    "#plt.show()\n",
    "\n",
    "print(\"\\tBinary Accuracy: \", BA1, \"\\tF1 Score: \", f11)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822dc7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_names = [\"results/MfracNN_Results.np\"]\n",
    "#results1 = np.concatenate((x1_v2[np.newaxis],y1_v2[np.newaxis],th1_v2[np.newaxis]),axis=0)\n",
    "#results = [results1]\n",
    "#for i, file in enumerate(file_names):\n",
    "#    with open(file, 'wb') as f:\n",
    "#        np.save(f, results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8670008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)\n",
    "print(\"Trainable Parameters :\", np.sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "#model = torch.load(\"PUFNN.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ### Visualize Attention Scores\n",
    "\n",
    "plot_events=5\n",
    "for i in range(plot_events):\n",
    "    # Get Model Predictions\n",
    "    model.eval()\n",
    "    output, jet_weights, trk_weights, cross_weights = model(X_test[i][0].to(device), X_test[i][1].to(device), X_test[i][2].to(device))\n",
    "\n",
    "    num_jets = len(output)\n",
    "\n",
    "    ticks = [x for x in range(num_jets)]\n",
    "    jet_labels= [round(float(label.detach().cpu().numpy()[0]),2) for label in y_test[i]]\n",
    "\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(15,15))\n",
    "    im0 = ax1.imshow(jet_weights.detach().cpu().numpy())\n",
    "    ax1.set_xticks(ticks, jet_labels)\n",
    "    ax1.set_yticks(ticks, jet_labels)\n",
    "    ax1.set_title(\"Attention Scores Between Jets and tracks\")\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im0, cax=cax, orientation='vertical')\n",
    "\"\"\"\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
